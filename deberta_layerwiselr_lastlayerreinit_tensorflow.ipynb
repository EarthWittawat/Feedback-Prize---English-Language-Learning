{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EarthWittawat/Feedback-Prize---English-Language-Learning/blob/main/deberta_layerwiselr_lastlayerreinit_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "print(f'TF version: {tf.__version__}')\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import transformers\n",
        "print(f'transformers version: {transformers.__version__}')\n",
        "from transformers import logging as hf_logging\n",
        "hf_logging.set_verbosity_error()\n",
        "\n",
        "import sys\n",
        "sys.path.append('../input/iterativestratification')\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-10-26T06:40:13.473602Z",
          "iopub.execute_input": "2022-10-26T06:40:13.473947Z",
          "iopub.status.idle": "2022-10-26T06:40:13.772619Z",
          "shell.execute_reply.started": "2022-10-26T06:40:13.473917Z",
          "shell.execute_reply": "2022-10-26T06:40:13.771641Z"
        },
        "trusted": true,
        "id": "AshdMGH0oKIS",
        "outputId": "21c4c670-1385-47ba-bc11-a468c89672fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "TF version: 2.6.4\ntransformers version: 4.20.1\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "#     os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "set_seed(42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T06:40:19.095928Z",
          "iopub.execute_input": "2022-10-26T06:40:19.096430Z",
          "iopub.status.idle": "2022-10-26T06:40:19.102972Z",
          "shell.execute_reply.started": "2022-10-26T06:40:19.096389Z",
          "shell.execute_reply": "2022-10-26T06:40:19.102057Z"
        },
        "trusted": true,
        "id": "jsvFZt88oKIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load DataFrame"
      ],
      "metadata": {
        "id": "L-_Ta1e8oKId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('../input/feedback-prize-english-language-learning/train.csv')\n",
        "display(df.head())\n",
        "print('\\n---------DataFrame Summary---------')\n",
        "df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T06:40:20.927149Z",
          "iopub.execute_input": "2022-10-26T06:40:20.927907Z",
          "iopub.status.idle": "2022-10-26T06:40:21.175621Z",
          "shell.execute_reply.started": "2022-10-26T06:40:20.927857Z",
          "shell.execute_reply": "2022-10-26T06:40:21.174526Z"
        },
        "trusted": true,
        "id": "1aG49rUqoKIg",
        "outputId": "06ccc587-6d03-4948-fdbd-a9fef9582653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "        text_id                                          full_text  cohesion  \\\n0  0016926B079C  I think that students would benefit from learn...       3.5   \n1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n3  003885A45F42  The best time in life is when you become yours...       4.5   \n4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n\n   syntax  vocabulary  phraseology  grammar  conventions  \n0     3.5         3.0          3.0      4.0          3.0  \n1     2.5         3.0          2.0      2.0          2.5  \n2     3.5         3.0          3.0      3.0          2.5  \n3     4.5         4.5          4.5      4.0          5.0  \n4     3.0         3.0          3.0      2.5          2.5  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>full_text</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0016926B079C</td>\n      <td>I think that students would benefit from learn...</td>\n      <td>3.5</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0022683E9EA5</td>\n      <td>When a problem is a change you have to let it ...</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00299B378633</td>\n      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003885A45F42</td>\n      <td>The best time in life is when you become yours...</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0049B1DF5CCC</td>\n      <td>Small act of kindness can impact in other peop...</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>2.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n---------DataFrame Summary---------\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3911 entries, 0 to 3910\nData columns (total 8 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   text_id      3911 non-null   object \n 1   full_text    3911 non-null   object \n 2   cohesion     3911 non-null   float64\n 3   syntax       3911 non-null   float64\n 4   vocabulary   3911 non-null   float64\n 5   phraseology  3911 non-null   float64\n 6   grammar      3911 non-null   float64\n 7   conventions  3911 non-null   float64\ndtypes: float64(6), object(2)\nmemory usage: 244.6+ KB\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CV Split"
      ],
      "metadata": {
        "id": "i5q0aHXAoKIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_FOLD = 5\n",
        "TARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "\n",
        "skf = MultilabelStratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=42)\n",
        "for n, (train_index, val_index) in enumerate(skf.split(df, df[TARGET_COLS])):\n",
        "    df.loc[val_index, 'fold'] = int(n)\n",
        "df['fold'] = df['fold'].astype(int)\n",
        "df['fold'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T06:40:23.215467Z",
          "iopub.execute_input": "2022-10-26T06:40:23.215832Z",
          "iopub.status.idle": "2022-10-26T06:40:23.351232Z",
          "shell.execute_reply.started": "2022-10-26T06:40:23.215802Z",
          "shell.execute_reply": "2022-10-26T06:40:23.350184Z"
        },
        "trusted": true,
        "id": "2vXeaxOOoKIj",
        "outputId": "bc200161-4280-45b8-d1ec-9b50cbd45603"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "1    783\n0    782\n4    782\n3    782\n2    782\nName: fold, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('./df_folds.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T06:43:56.746146Z",
          "iopub.execute_input": "2022-10-26T06:43:56.747085Z",
          "iopub.status.idle": "2022-10-26T06:43:56.918746Z",
          "shell.execute_reply.started": "2022-10-26T06:43:56.747045Z",
          "shell.execute_reply": "2022-10-26T06:43:56.917758Z"
        },
        "trusted": true,
        "id": "0wS82TTXoKIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Config"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T06:47:00.280577Z",
          "iopub.execute_input": "2022-10-26T06:47:00.280924Z",
          "iopub.status.idle": "2022-10-26T06:47:00.391654Z",
          "shell.execute_reply.started": "2022-10-26T06:47:00.280895Z",
          "shell.execute_reply": "2022-10-26T06:47:00.390693Z"
        },
        "id": "2hfckZsRoKIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 512\n",
        "BATCH_SIZE = 4\n",
        "DEBERTA_MODEL = \"../input/debertav3base\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T06:48:08.655518Z",
          "iopub.execute_input": "2022-10-26T06:48:08.655874Z",
          "iopub.status.idle": "2022-10-26T06:48:08.660686Z",
          "shell.execute_reply.started": "2022-10-26T06:48:08.655845Z",
          "shell.execute_reply": "2022-10-26T06:48:08.659606Z"
        },
        "trusted": true,
        "id": "DHovpj7ioKIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(DEBERTA_MODEL)\n",
        "tokenizer.save_pretrained('./tokenizer/')\n",
        "\n",
        "cfg = transformers.AutoConfig.from_pretrained(DEBERTA_MODEL, output_hidden_states=True)\n",
        "cfg.hidden_dropout_prob = 0\n",
        "cfg.attention_probs_dropout_prob = 0\n",
        "cfg.save_pretrained('./tokenizer/')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T06:48:14.005255Z",
          "iopub.execute_input": "2022-10-26T06:48:14.005635Z",
          "iopub.status.idle": "2022-10-26T06:48:16.370709Z",
          "shell.execute_reply.started": "2022-10-26T06:48:14.005603Z",
          "shell.execute_reply": "2022-10-26T06:48:16.369635Z"
        },
        "trusted": true,
        "id": "3OwcKg7koKIo",
        "outputId": "a8f2f854-7645-41f7-a449-664a0a37a869"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Process Function"
      ],
      "metadata": {
        "id": "3uHwOsW6oKIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deberta_encode(texts, tokenizer=tokenizer):\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for text in texts.tolist():\n",
        "        token = tokenizer(text, \n",
        "                          add_special_tokens=True, \n",
        "                          max_length=MAX_LENGTH, \n",
        "                          return_attention_mask=True, \n",
        "                          return_tensors=\"np\", \n",
        "                          truncation=True, \n",
        "                          padding='max_length')\n",
        "        input_ids.append(token['input_ids'][0])\n",
        "        attention_mask.append(token['attention_mask'][0])\n",
        "    \n",
        "    return np.array(input_ids, dtype=\"int32\"), np.array(attention_mask, dtype=\"int32\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T06:48:39.087006Z",
          "iopub.execute_input": "2022-10-26T06:48:39.087380Z",
          "iopub.status.idle": "2022-10-26T06:48:39.095717Z",
          "shell.execute_reply.started": "2022-10-26T06:48:39.087348Z",
          "shell.execute_reply": "2022-10-26T06:48:39.094347Z"
        },
        "trusted": true,
        "id": "U70e-j06oKIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(df):\n",
        "    inputs = deberta_encode(df['full_text'])\n",
        "    targets = np.array(df[TARGET_COLS], dtype=\"float32\")\n",
        "    return inputs, targets"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T06:48:45.226731Z",
          "iopub.execute_input": "2022-10-26T06:48:45.227080Z",
          "iopub.status.idle": "2022-10-26T06:48:45.232447Z",
          "shell.execute_reply.started": "2022-10-26T06:48:45.227050Z",
          "shell.execute_reply": "2022-10-26T06:48:45.230937Z"
        },
        "trusted": true,
        "id": "EvNc6tsVoKIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "itw3TirtoKIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MeanPool(tf.keras.layers.Layer):\n",
        "    def call(self, inputs, mask=None):\n",
        "        broadcast_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n",
        "        embedding_sum = tf.reduce_sum(inputs * broadcast_mask, axis=1)\n",
        "        mask_sum = tf.reduce_sum(broadcast_mask, axis=1)\n",
        "        mask_sum = tf.math.maximum(mask_sum, tf.constant([1e-9]))\n",
        "        return embedding_sum / mask_sum"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T06:48:57.674661Z",
          "iopub.execute_input": "2022-10-26T06:48:57.675019Z",
          "iopub.status.idle": "2022-10-26T06:48:57.681025Z",
          "shell.execute_reply.started": "2022-10-26T06:48:57.674987Z",
          "shell.execute_reply": "2022-10-26T06:48:57.679624Z"
        },
        "trusted": true,
        "id": "zBtGGEZuoKIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightsSumOne(tf.keras.constraints.Constraint):\n",
        "    def __call__(self, w):\n",
        "        return tf.nn.softmax(w, axis=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T06:49:02.896707Z",
          "iopub.execute_input": "2022-10-26T06:49:02.897061Z",
          "iopub.status.idle": "2022-10-26T06:49:02.902702Z",
          "shell.execute_reply.started": "2022-10-26T06:49:02.897031Z",
          "shell.execute_reply": "2022-10-26T06:49:02.901424Z"
        },
        "trusted": true,
        "id": "mwhg-nFvoKIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    input_ids = tf.keras.layers.Input(\n",
        "        shape=(MAX_LENGTH,), dtype=tf.int32, name=\"input_ids\"\n",
        "    )\n",
        "    \n",
        "    attention_masks = tf.keras.layers.Input(\n",
        "        shape=(MAX_LENGTH,), dtype=tf.int32, name=\"attention_masks\"\n",
        "    )\n",
        "   \n",
        "    deberta_model = transformers.TFAutoModel.from_pretrained(DEBERTA_MODEL, config=cfg)\n",
        "    \n",
        "    #Last Layer Reinitialization or Partially Reinitialization\n",
        "#     Uncommon next three lines to check deberta encoder block\n",
        "#     print('DeBERTa Encoder Block:')\n",
        "#     for layer in deberta_model.deberta.encoder.layer:\n",
        "#         print(layer)\n",
        "        \n",
        "    REINIT_LAYERS = 1\n",
        "    normal_initializer = tf.keras.initializers.GlorotUniform()\n",
        "    zeros_initializer = tf.keras.initializers.Zeros()\n",
        "    ones_initializer = tf.keras.initializers.Ones()\n",
        "\n",
        "#     print(f'\\nRe-initializing encoder block:')\n",
        "    for encoder_block in deberta_model.deberta.encoder.layer[-REINIT_LAYERS:]:\n",
        "#         print(f'{encoder_block}')\n",
        "        for layer in encoder_block.submodules:\n",
        "            if isinstance(layer, tf.keras.layers.Dense):\n",
        "                layer.kernel.assign(normal_initializer(shape=layer.kernel.shape, dtype=layer.kernel.dtype))\n",
        "                if layer.bias is not None:\n",
        "                    layer.bias.assign(zeros_initializer(shape=layer.bias.shape, dtype=layer.bias.dtype))\n",
        "\n",
        "            elif isinstance(layer, tf.keras.layers.LayerNormalization):\n",
        "                layer.beta.assign(zeros_initializer(shape=layer.beta.shape, dtype=layer.beta.dtype))\n",
        "                layer.gamma.assign(ones_initializer(shape=layer.gamma.shape, dtype=layer.gamma.dtype))\n",
        "\n",
        "    deberta_output = deberta_model.deberta(\n",
        "        input_ids, attention_mask=attention_masks\n",
        "    )\n",
        "    hidden_states = deberta_output.hidden_states\n",
        "    \n",
        "    #WeightedLayerPool + MeanPool of the last 4 hidden states\n",
        "    stack_meanpool = tf.stack(\n",
        "        [MeanPool()(hidden_s, mask=attention_masks) for hidden_s in hidden_states[-4:]], \n",
        "        axis=2)\n",
        "    \n",
        "    weighted_layer_pool = layers.Dense(1,\n",
        "                                       use_bias=False,\n",
        "                                       kernel_constraint=WeightsSumOne())(stack_meanpool)\n",
        "    \n",
        "    weighted_layer_pool = tf.squeeze(weighted_layer_pool, axis=-1)\n",
        "    \n",
        "    x = layers.Dense(6, activation='sigmoid')(weighted_layer_pool)\n",
        "    output = layers.Rescaling(scale=4.0, offset=1.0)(x)\n",
        "    model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output)\n",
        "    \n",
        "    #Compile model with Layer-wise Learning Rate Decay\n",
        "    layer_list = [deberta_model.deberta.embeddings] + list(deberta_model.deberta.encoder.layer)\n",
        "    layer_list.reverse()\n",
        "    \n",
        "    INIT_LR = 1e-5\n",
        "    LLRDR = 0.9\n",
        "    LR_SCH_DECAY_STEPS = 1600 # 2 * len(train_df) // BATCH_SIZE\n",
        "    \n",
        "    lr_schedules = [tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=INIT_LR * LLRDR ** i, \n",
        "        decay_steps=LR_SCH_DECAY_STEPS, \n",
        "        decay_rate=0.3) for i in range(len(layer_list))]\n",
        "    lr_schedule_head = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=1e-4, \n",
        "        decay_steps=LR_SCH_DECAY_STEPS, \n",
        "        decay_rate=0.3)\n",
        "    \n",
        "    optimizers = [tf.keras.optimizers.Adam(learning_rate=lr_sch) for lr_sch in lr_schedules]\n",
        "    \n",
        "    optimizers_and_layers = [(tf.keras.optimizers.Adam(learning_rate=lr_schedule_head), model.layers[-4:])] +\\\n",
        "        list(zip(optimizers, layer_list))\n",
        "    \n",
        "#     Uncomment next three lines to check optimizers_and_layers\n",
        "#     print('\\nLayer-wise Learning Rate Decay Initial LR:')\n",
        "#     for o,l in optimizers_and_layers:\n",
        "#         print(f'{o._decayed_lr(\"float32\").numpy()} for {l}')\n",
        "        \n",
        "    optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n",
        "    \n",
        "    model.compile(optimizer=optimizer,\n",
        "                 loss='huber_loss',\n",
        "                 metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
        "                 )\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T06:49:09.699380Z",
          "iopub.execute_input": "2022-10-26T06:49:09.699745Z",
          "iopub.status.idle": "2022-10-26T06:49:09.717072Z",
          "shell.execute_reply.started": "2022-10-26T06:49:09.699713Z",
          "shell.execute_reply": "2022-10-26T06:49:09.715953Z"
        },
        "trusted": true,
        "id": "dLr1h9CSoKIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = get_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T06:49:15.660643Z",
          "iopub.execute_input": "2022-10-26T06:49:15.660991Z",
          "iopub.status.idle": "2022-10-26T06:49:42.239890Z",
          "shell.execute_reply.started": "2022-10-26T06:49:15.660963Z",
          "shell.execute_reply": "2022-10-26T06:49:42.238853Z"
        },
        "trusted": true,
        "id": "cWpxU8sqoKIt",
        "outputId": "aa773816-f7f5-41a9-f5bb-8f9cecf633a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2022-10-26 06:49:17.360650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-26 06:49:17.361705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-26 06:49:17.362426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-26 06:49:17.363298: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-10-26 06:49:17.363589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-26 06:49:17.364288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-26 06:49:17.364915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-26 06:49:22.322323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-26 06:49:22.323108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-26 06:49:22.323839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-26 06:49:22.324448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15043 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n2022-10-26 06:49:32.121641: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_ids (InputLayer)          [(None, 512)]        0                                            \n__________________________________________________________________________________________________\nattention_masks (InputLayer)    [(None, 512)]        0                                            \n__________________________________________________________________________________________________\ndeberta (TFDebertaV2MainLayer)  TFBaseModelOutput(la 183831552   input_ids[0][0]                  \n                                                                 attention_masks[0][0]            \n__________________________________________________________________________________________________\nmean_pool (MeanPool)            (None, 768)          0           deberta[0][9]                    \n                                                                 attention_masks[0][0]            \n__________________________________________________________________________________________________\nmean_pool_1 (MeanPool)          (None, 768)          0           deberta[0][10]                   \n                                                                 attention_masks[0][0]            \n__________________________________________________________________________________________________\nmean_pool_2 (MeanPool)          (None, 768)          0           deberta[0][11]                   \n                                                                 attention_masks[0][0]            \n__________________________________________________________________________________________________\nmean_pool_3 (MeanPool)          (None, 768)          0           deberta[0][12]                   \n                                                                 attention_masks[0][0]            \n__________________________________________________________________________________________________\ntf.stack (TFOpLambda)           (None, 768, 4)       0           mean_pool[0][0]                  \n                                                                 mean_pool_1[0][0]                \n                                                                 mean_pool_2[0][0]                \n                                                                 mean_pool_3[0][0]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 768, 1)       4           tf.stack[0][0]                   \n__________________________________________________________________________________________________\ntf.compat.v1.squeeze (TFOpLambd (None, 768)          0           dense[0][0]                      \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 6)            4614        tf.compat.v1.squeeze[0][0]       \n__________________________________________________________________________________________________\nrescaling (Rescaling)           (None, 6)            0           dense_1[0][0]                    \n==================================================================================================\nTotal params: 183,836,170\nTrainable params: 183,836,170\nNon-trainable params: 0\n__________________________________________________________________________________________________\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_rmses = []\n",
        "for fold in range(N_FOLD):\n",
        "    print(f'\\n-----------FOLD {fold} ------------')\n",
        "    \n",
        "    #Create dataset\n",
        "    train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
        "    valid_df = df[df['fold'] == fold].reset_index(drop=True)\n",
        "    train_dataset = get_dataset(train_df)\n",
        "    valid_dataset = get_dataset(valid_df)\n",
        "    \n",
        "    print('Data prepared.')\n",
        "    print(f'Training data input_ids shape: {train_dataset[0][0].shape} dtype: {train_dataset[0][0].dtype}') \n",
        "    print(f'Training data attention_mask shape: {train_dataset[0][1].shape} dtype: {train_dataset[0][1].dtype}')\n",
        "    print(f'Training data targets shape: {train_dataset[1].shape} dtype: {train_dataset[1].dtype}')\n",
        "    print(f'Validation data input_ids shape: {valid_dataset[0][0].shape} dtype: {valid_dataset[0][0].dtype}')\n",
        "    print(f'Validation data attention_mask shape: {valid_dataset[0][1].shape} dtype: {valid_dataset[0][1].dtype}')\n",
        "    print(f'Validation data targets shape: {valid_dataset[1].shape} dtype: {valid_dataset[1].dtype}')\n",
        "    \n",
        "    #Create model\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = get_model()\n",
        "    print('Model prepared.')\n",
        "    \n",
        "    #Training model\n",
        "    print('Start training...')\n",
        "    callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(f\"best_model_fold{fold}.h5\",\n",
        "                                       monitor=\"val_loss\",\n",
        "                                       mode=\"min\",\n",
        "                                       save_best_only=True,\n",
        "                                       verbose=1,\n",
        "                                       save_weights_only=True,),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                     min_delta=1e-5, \n",
        "                                     patience=3, \n",
        "                                     verbose=1,\n",
        "                                     mode='min',)\n",
        "    ]\n",
        "    history = model.fit(x=train_dataset[0],\n",
        "                        y=train_dataset[1],\n",
        "                        validation_data=valid_dataset, \n",
        "                        epochs=10,\n",
        "                        shuffle=True,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        callbacks=callbacks\n",
        "                       )\n",
        "    \n",
        "    valid_rmses.append(np.min(history.history['val_root_mean_squared_error']))\n",
        "    print('Training finished.')\n",
        "    del train_dataset, valid_dataset, train_df, valid_df\n",
        "    gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T06:49:42.242498Z",
          "iopub.execute_input": "2022-10-26T06:49:42.243389Z",
          "iopub.status.idle": "2022-10-26T10:15:31.431480Z",
          "shell.execute_reply.started": "2022-10-26T06:49:42.243349Z",
          "shell.execute_reply": "2022-10-26T10:15:31.430363Z"
        },
        "trusted": true,
        "id": "toN1dUaQoKIu",
        "outputId": "708068c9-979a-4ea8-8742-94d874c5400a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n-----------FOLD 0 ------------\nData prepared.\nTraining data input_ids shape: (3129, 512) dtype: int32\nTraining data attention_mask shape: (3129, 512) dtype: int32\nTraining data targets shape: (3129, 6) dtype: float32\nValidation data input_ids shape: (782, 512) dtype: int32\nValidation data attention_mask shape: (782, 512) dtype: int32\nValidation data targets shape: (782, 6) dtype: float32\nModel prepared.\nStart training...\nEpoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "2022-10-26 06:50:02.390320: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "783/783 [==============================] - 418s 476ms/step - loss: 0.1230 - root_mean_squared_error: 0.4997 - val_loss: 0.1047 - val_root_mean_squared_error: 0.4588\n\nEpoch 00001: val_loss improved from inf to 0.10474, saving model to best_model_fold0.h5\nEpoch 2/10\n783/783 [==============================] - 367s 468ms/step - loss: 0.1000 - root_mean_squared_error: 0.4485 - val_loss: 0.1070 - val_root_mean_squared_error: 0.4637\n\nEpoch 00002: val_loss did not improve from 0.10474\nEpoch 3/10\n783/783 [==============================] - 366s 468ms/step - loss: 0.0916 - root_mean_squared_error: 0.4289 - val_loss: 0.1011 - val_root_mean_squared_error: 0.4506\n\nEpoch 00003: val_loss improved from 0.10474 to 0.10110, saving model to best_model_fold0.h5\nEpoch 4/10\n783/783 [==============================] - 366s 468ms/step - loss: 0.0865 - root_mean_squared_error: 0.4164 - val_loss: 0.1025 - val_root_mean_squared_error: 0.4537\n\nEpoch 00004: val_loss did not improve from 0.10110\nEpoch 5/10\n783/783 [==============================] - 366s 467ms/step - loss: 0.0832 - root_mean_squared_error: 0.4084 - val_loss: 0.1021 - val_root_mean_squared_error: 0.4527\n\nEpoch 00005: val_loss did not improve from 0.10110\nEpoch 6/10\n783/783 [==============================] - 366s 468ms/step - loss: 0.0813 - root_mean_squared_error: 0.4037 - val_loss: 0.1026 - val_root_mean_squared_error: 0.4539\n\nEpoch 00006: val_loss did not improve from 0.10110\nEpoch 00006: early stopping\nTraining finished.\n\n-----------FOLD 1 ------------\nData prepared.\nTraining data input_ids shape: (3128, 512) dtype: int32\nTraining data attention_mask shape: (3128, 512) dtype: int32\nTraining data targets shape: (3128, 6) dtype: float32\nValidation data input_ids shape: (783, 512) dtype: int32\nValidation data attention_mask shape: (783, 512) dtype: int32\nValidation data targets shape: (783, 6) dtype: float32\nModel prepared.\nStart training...\nEpoch 1/10\n782/782 [==============================] - 382s 446ms/step - loss: 0.1240 - root_mean_squared_error: 0.5017 - val_loss: 0.1072 - val_root_mean_squared_error: 0.4647\n\nEpoch 00001: val_loss improved from inf to 0.10719, saving model to best_model_fold1.h5\nEpoch 2/10\n782/782 [==============================] - 342s 438ms/step - loss: 0.0991 - root_mean_squared_error: 0.4463 - val_loss: 0.1061 - val_root_mean_squared_error: 0.4620\n\nEpoch 00002: val_loss improved from 0.10719 to 0.10606, saving model to best_model_fold1.h5\nEpoch 3/10\n782/782 [==============================] - 342s 437ms/step - loss: 0.0901 - root_mean_squared_error: 0.4250 - val_loss: 0.1068 - val_root_mean_squared_error: 0.4638\n\nEpoch 00003: val_loss did not improve from 0.10606\nEpoch 4/10\n782/782 [==============================] - 342s 438ms/step - loss: 0.0854 - root_mean_squared_error: 0.4138 - val_loss: 0.1054 - val_root_mean_squared_error: 0.4605\n\nEpoch 00004: val_loss improved from 0.10606 to 0.10541, saving model to best_model_fold1.h5\nEpoch 5/10\n782/782 [==============================] - 342s 438ms/step - loss: 0.0820 - root_mean_squared_error: 0.4053 - val_loss: 0.1048 - val_root_mean_squared_error: 0.4591\n\nEpoch 00005: val_loss improved from 0.10541 to 0.10475, saving model to best_model_fold1.h5\nEpoch 6/10\n782/782 [==============================] - 342s 438ms/step - loss: 0.0800 - root_mean_squared_error: 0.4003 - val_loss: 0.1058 - val_root_mean_squared_error: 0.4614\n\nEpoch 00006: val_loss did not improve from 0.10475\nEpoch 7/10\n782/782 [==============================] - 342s 438ms/step - loss: 0.0790 - root_mean_squared_error: 0.3979 - val_loss: 0.1049 - val_root_mean_squared_error: 0.4595\n\nEpoch 00007: val_loss did not improve from 0.10475\nEpoch 8/10\n782/782 [==============================] - 342s 438ms/step - loss: 0.0783 - root_mean_squared_error: 0.3962 - val_loss: 0.1049 - val_root_mean_squared_error: 0.4595\n\nEpoch 00008: val_loss did not improve from 0.10475\nEpoch 00008: early stopping\nTraining finished.\n\n-----------FOLD 2 ------------\nData prepared.\nTraining data input_ids shape: (3129, 512) dtype: int32\nTraining data attention_mask shape: (3129, 512) dtype: int32\nTraining data targets shape: (3129, 6) dtype: float32\nValidation data input_ids shape: (782, 512) dtype: int32\nValidation data attention_mask shape: (782, 512) dtype: int32\nValidation data targets shape: (782, 6) dtype: float32\nModel prepared.\nStart training...\nEpoch 1/10\n783/783 [==============================] - 415s 475ms/step - loss: 0.1229 - root_mean_squared_error: 0.4991 - val_loss: 0.1131 - val_root_mean_squared_error: 0.4780\n\nEpoch 00001: val_loss improved from inf to 0.11313, saving model to best_model_fold2.h5\nEpoch 2/10\n783/783 [==============================] - 365s 467ms/step - loss: 0.0987 - root_mean_squared_error: 0.4453 - val_loss: 0.1061 - val_root_mean_squared_error: 0.4626\n\nEpoch 00002: val_loss improved from 0.11313 to 0.10613, saving model to best_model_fold2.h5\nEpoch 3/10\n783/783 [==============================] - 365s 467ms/step - loss: 0.0906 - root_mean_squared_error: 0.4265 - val_loss: 0.1106 - val_root_mean_squared_error: 0.4731\n\nEpoch 00003: val_loss did not improve from 0.10613\nEpoch 4/10\n783/783 [==============================] - 365s 466ms/step - loss: 0.0856 - root_mean_squared_error: 0.4143 - val_loss: 0.1070 - val_root_mean_squared_error: 0.4647\n\nEpoch 00004: val_loss did not improve from 0.10613\nEpoch 5/10\n783/783 [==============================] - 365s 466ms/step - loss: 0.0823 - root_mean_squared_error: 0.4061 - val_loss: 0.1072 - val_root_mean_squared_error: 0.4654\n\nEpoch 00005: val_loss did not improve from 0.10613\nEpoch 00005: early stopping\nTraining finished.\n\n-----------FOLD 3 ------------\nData prepared.\nTraining data input_ids shape: (3129, 512) dtype: int32\nTraining data attention_mask shape: (3129, 512) dtype: int32\nTraining data targets shape: (3129, 6) dtype: float32\nValidation data input_ids shape: (782, 512) dtype: int32\nValidation data attention_mask shape: (782, 512) dtype: int32\nValidation data targets shape: (782, 6) dtype: float32\nModel prepared.\nStart training...\nEpoch 1/10\n783/783 [==============================] - 415s 476ms/step - loss: 0.1214 - root_mean_squared_error: 0.4957 - val_loss: 0.1091 - val_root_mean_squared_error: 0.4684\n\nEpoch 00001: val_loss improved from inf to 0.10907, saving model to best_model_fold3.h5\nEpoch 2/10\n783/783 [==============================] - 366s 467ms/step - loss: 0.0982 - root_mean_squared_error: 0.4443 - val_loss: 0.1040 - val_root_mean_squared_error: 0.4571\n\nEpoch 00002: val_loss improved from 0.10907 to 0.10395, saving model to best_model_fold3.h5\nEpoch 3/10\n783/783 [==============================] - 366s 467ms/step - loss: 0.0901 - root_mean_squared_error: 0.4251 - val_loss: 0.1034 - val_root_mean_squared_error: 0.4557\n\nEpoch 00003: val_loss improved from 0.10395 to 0.10337, saving model to best_model_fold3.h5\nEpoch 4/10\n783/783 [==============================] - 365s 467ms/step - loss: 0.0849 - root_mean_squared_error: 0.4125 - val_loss: 0.1030 - val_root_mean_squared_error: 0.4550\n\nEpoch 00004: val_loss improved from 0.10337 to 0.10301, saving model to best_model_fold3.h5\nEpoch 5/10\n783/783 [==============================] - 366s 467ms/step - loss: 0.0818 - root_mean_squared_error: 0.4049 - val_loss: 0.1030 - val_root_mean_squared_error: 0.4550\n\nEpoch 00005: val_loss did not improve from 0.10301\nEpoch 6/10\n783/783 [==============================] - 366s 467ms/step - loss: 0.0799 - root_mean_squared_error: 0.4001 - val_loss: 0.1036 - val_root_mean_squared_error: 0.4563\n\nEpoch 00006: val_loss did not improve from 0.10301\nEpoch 7/10\n783/783 [==============================] - 366s 468ms/step - loss: 0.0788 - root_mean_squared_error: 0.3973 - val_loss: 0.1033 - val_root_mean_squared_error: 0.4557\n\nEpoch 00007: val_loss did not improve from 0.10301\nEpoch 00007: early stopping\nTraining finished.\n\n-----------FOLD 4 ------------\nData prepared.\nTraining data input_ids shape: (3129, 512) dtype: int32\nTraining data attention_mask shape: (3129, 512) dtype: int32\nTraining data targets shape: (3129, 6) dtype: float32\nValidation data input_ids shape: (782, 512) dtype: int32\nValidation data attention_mask shape: (782, 512) dtype: int32\nValidation data targets shape: (782, 6) dtype: float32\nModel prepared.\nStart training...\nEpoch 1/10\n783/783 [==============================] - 416s 477ms/step - loss: 0.1217 - root_mean_squared_error: 0.4961 - val_loss: 0.1055 - val_root_mean_squared_error: 0.4612\n\nEpoch 00001: val_loss improved from inf to 0.10550, saving model to best_model_fold4.h5\nEpoch 2/10\n783/783 [==============================] - 367s 469ms/step - loss: 0.0989 - root_mean_squared_error: 0.4459 - val_loss: 0.1040 - val_root_mean_squared_error: 0.4576\n\nEpoch 00002: val_loss improved from 0.10550 to 0.10404, saving model to best_model_fold4.h5\nEpoch 3/10\n783/783 [==============================] - 367s 469ms/step - loss: 0.0913 - root_mean_squared_error: 0.4280 - val_loss: 0.1033 - val_root_mean_squared_error: 0.4558\n\nEpoch 00003: val_loss improved from 0.10404 to 0.10327, saving model to best_model_fold4.h5\nEpoch 4/10\n783/783 [==============================] - 367s 468ms/step - loss: 0.0857 - root_mean_squared_error: 0.4145 - val_loss: 0.1016 - val_root_mean_squared_error: 0.4522\n\nEpoch 00004: val_loss improved from 0.10327 to 0.10158, saving model to best_model_fold4.h5\nEpoch 5/10\n783/783 [==============================] - 367s 469ms/step - loss: 0.0827 - root_mean_squared_error: 0.4072 - val_loss: 0.1017 - val_root_mean_squared_error: 0.4524\n\nEpoch 00005: val_loss did not improve from 0.10158\nEpoch 6/10\n783/783 [==============================] - 368s 471ms/step - loss: 0.0807 - root_mean_squared_error: 0.4022 - val_loss: 0.1022 - val_root_mean_squared_error: 0.4535\n\nEpoch 00006: val_loss did not improve from 0.10158\nEpoch 7/10\n783/783 [==============================] - 366s 467ms/step - loss: 0.0797 - root_mean_squared_error: 0.3997 - val_loss: 0.1016 - val_root_mean_squared_error: 0.4522\n\nEpoch 00007: val_loss did not improve from 0.10158\nEpoch 00007: early stopping\nTraining finished.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{len(valid_rmses)} Folds validation RMSE:\\n{valid_rmses}')\n",
        "print(f'Local CV Average score: {np.mean(valid_rmses)}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T10:15:31.439839Z",
          "iopub.execute_input": "2022-10-26T10:15:31.442167Z",
          "iopub.status.idle": "2022-10-26T10:15:31.506487Z",
          "shell.execute_reply.started": "2022-10-26T10:15:31.442121Z",
          "shell.execute_reply": "2022-10-26T10:15:31.504693Z"
        },
        "trusted": true,
        "id": "8Vc0daXHoKIv",
        "outputId": "e0ae199b-3900-4bc3-f160-06cc52233c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "5 Folds validation RMSE:\n[0.4505632519721985, 0.459128201007843, 0.46264514327049255, 0.45495015382766724, 0.45215508341789246]\nLocal CV Average score: 0.45588836669921873\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\n",
        "test_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T10:20:36.242086Z",
          "iopub.execute_input": "2022-10-26T10:20:36.242882Z",
          "iopub.status.idle": "2022-10-26T10:20:36.264706Z",
          "shell.execute_reply.started": "2022-10-26T10:20:36.242836Z",
          "shell.execute_reply": "2022-10-26T10:20:36.263076Z"
        },
        "trusted": true,
        "id": "ZyrZuAktoKIv",
        "outputId": "c11c081c-f6ae-48d8-81b6-2cf05f895cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "        text_id                                          full_text\n0  0000C359D63E  when a person has no experience on a job their...\n1  000BAD50D026  Do you think students would benefit from being...\n2  00367BB2546B  Thomas Jefferson once states that \"it is wonde...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>full_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>when a person has no experience on a job their...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>Do you think students would benefit from being...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>Thomas Jefferson once states that \"it is wonde...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = deberta_encode(test_df['full_text'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T10:20:47.299957Z",
          "iopub.execute_input": "2022-10-26T10:20:47.300230Z",
          "iopub.status.idle": "2022-10-26T10:20:47.316195Z",
          "shell.execute_reply.started": "2022-10-26T10:20:47.300204Z",
          "shell.execute_reply": "2022-10-26T10:20:47.315174Z"
        },
        "trusted": true,
        "id": "HR77b3BeoKIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fold_preds = []\n",
        "for fold in range(N_FOLD):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = get_model()\n",
        "    model.load_weights(f'best_model_fold{fold}.h5')\n",
        "    print(f'\\nFold {fold} inference...')\n",
        "    pred = model.predict(test_dataset, batch_size=BATCH_SIZE)\n",
        "    fold_preds.append(pred)\n",
        "    gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T10:28:42.131398Z",
          "iopub.execute_input": "2022-10-26T10:28:42.131703Z",
          "iopub.status.idle": "2022-10-26T10:30:29.688563Z",
          "shell.execute_reply.started": "2022-10-26T10:28:42.131675Z",
          "shell.execute_reply": "2022-10-26T10:30:29.687524Z"
        },
        "trusted": true,
        "id": "BhRZroQjoKIw",
        "outputId": "a243d009-6955-42a6-b0d9-2de7025b14dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nFold 0 inference...\n\nFold 1 inference...\n\nFold 2 inference...\n\nFold 3 inference...\n\nFold 4 inference...\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = np.mean(fold_preds, axis=0)\n",
        "preds = np.clip(preds, 1, 5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T10:30:29.716434Z",
          "iopub.execute_input": "2022-10-26T10:30:29.716761Z",
          "iopub.status.idle": "2022-10-26T10:30:29.746989Z",
          "shell.execute_reply.started": "2022-10-26T10:30:29.716733Z",
          "shell.execute_reply": "2022-10-26T10:30:29.745513Z"
        },
        "trusted": true,
        "id": "AamCeMkJoKIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df = pd.concat([test_df[['text_id']], pd.DataFrame(preds, columns=TARGET_COLS)], axis=1)\n",
        "sub_df.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T10:30:29.749008Z",
          "iopub.execute_input": "2022-10-26T10:30:29.749865Z",
          "iopub.status.idle": "2022-10-26T10:30:29.774226Z",
          "shell.execute_reply.started": "2022-10-26T10:30:29.749820Z",
          "shell.execute_reply": "2022-10-26T10:30:29.773402Z"
        },
        "trusted": true,
        "id": "yTRIH4NFoKIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-26T10:30:29.794557Z",
          "iopub.execute_input": "2022-10-26T10:30:29.794922Z",
          "iopub.status.idle": "2022-10-26T10:30:29.811215Z",
          "shell.execute_reply.started": "2022-10-26T10:30:29.794884Z",
          "shell.execute_reply": "2022-10-26T10:30:29.810297Z"
        },
        "trusted": true,
        "id": "WSJsqTcooKIx",
        "outputId": "342645e2-74d5-4c0f-e5e4-e8c323e0cc34"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.927250  2.759098    3.090907     2.983907  2.654530   \n1  000BAD50D026  2.697297  2.502063    2.743973     2.359509  2.121621   \n2  00367BB2546B  3.708724  3.460325    3.653716     3.513729  3.371320   \n\n   conventions  \n0     2.625527  \n1     2.607434  \n2     3.262542  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.927250</td>\n      <td>2.759098</td>\n      <td>3.090907</td>\n      <td>2.983907</td>\n      <td>2.654530</td>\n      <td>2.625527</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.697297</td>\n      <td>2.502063</td>\n      <td>2.743973</td>\n      <td>2.359509</td>\n      <td>2.121621</td>\n      <td>2.607434</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.708724</td>\n      <td>3.460325</td>\n      <td>3.653716</td>\n      <td>3.513729</td>\n      <td>3.371320</td>\n      <td>3.262542</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    }
  ]
}